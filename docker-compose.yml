version: "3.4"

services:
  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop2.7.4-java8
    volumes:
      - type: bind
        source: ./data/namenode
        target: /hadoop/dfs/name
    environment:
      CLUSTER_NAME: "test"
      CORE_CONF_fs_defaultFS: "hdfs://namenode:8020"
      CORE_CONF_hadoop_http_staticuser_user: "root"
      CORE_CONF_hadoop_proxyuser_hue_hosts: "*"
      CORE_CONF_hadoop_proxyuser_hue_groups: "*"
      HDFS_CONF_dfs_webhdfs_enabled: "true"
      HDFS_CONF_dfs_permissions_enabled: "false"
    deploy:
      mode: replicated
      replicas: 1
      restart_policy:
        condition: on-failure
      placement:
        constraints:
          - node.hostname == akswnc4.aksw.uni-leipzig.de
      labels:
        traefik.frontend.rule: namenode.sansa.aksw.org
        traefik.docker.network: sansa
        traefik.port: 50070
    ports:
      - 50070:50070

  datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop2.7.4-java8
    volumes:
      - type: bind
        source: ./data/datanode
        target: /hadoop/dfs/data
    environment:
      CORE_CONF_fs_defaultFS: "hdfs://namenode:8020"
      SERVICE_PRECONDITION: "namenode:50070"
    deploy:
      mode: replicated
      replicas: 3
      restart_policy:
        condition: on-failure

  spark-master:
    image: bde2020/spark-master:2.1.0-hadoop2.8-hive-java8
    environment:
      CORE_CONF_fs_defaultFS: "hdfs://namenode:8020"
      SERVICE_PRECONDITION: "namenode:50070 datanode:50075"
    deploy:
      mode: replicated
      replicas: 1
      restart_policy:
        condition: on-failure
      placement:
        constraints:
          - node.hostname == akswnc4.aksw.uni-leipzig.de
      labels:
        traefik.docker.network: sansa
        traefik.port: 8080
        traefik.frontend.rule: spark.sansa.aksw.org
    ports:
      - 8080:8080

  spark-worker:
    image: bde2020/spark-worker:2.1.0-hadoop2.8-hive-java8
    environment:
      SPARK_MASTER: "spark://spark-master:7077"
      CORE_CONF_fs_defaultFS: "hdfs://namenode:8020"
      SERVICE_PRECONDITION: "spark-master:8080"
    deploy:
      mode: replicated
      replicas: 3
      restart_policy:
        condition: on-failure
    ports:
      - 8081:8081

  hue:
    image: bde2020/hdfs-filebrowser:3.11
    environment:
      NAMENODE_HOST: "namenode"
      SPARK_MASTER: "spark://spark-master:7077"
      SERVICE_PRECONDITION: "spark-master:8080"
    deploy:
      mode: replicated
      replicas: 1
      restart_policy:
        condition: on-failure
      placement:
        constraints:
          - node.hostname == akswnc4.aksw.uni-leipzig.de
      labels:
        traefik.docker.network: sansa
        traefik.port: 8088
        traefik.frontend.rule: hdfs.sansa.aksw.org

  zeppelin:
    image: bde2020/zeppelin:0.0.1-zeppelin-0.7.1-hadoop-2.8.0-spark-2.1.0
    volumes:
      - type: bind
        source: ./notebook
        target: /opt/zeppelin/notebook
      - type: bind
        source: ./examples
        target: /opt/sansa-examples
    environment:
      CORE_CONF_fs_defaultFS: "hdfs://namenode:8020"
      SPARK_MASTER: "spark://spark-master:7077"
      MASTER: "spark://spark-master:7077"
      SPARK_SUBMIT_OPTIONS: "--jars /opt/sansa-examples/jars/sansa-examples-spark.jar --conf spark.serializer=org.apache.spark.serializer.KryoSerializer"
      SERVICE_PRECONDITION: "spark-master:8080"
    deploy:
       mode: replicated
       replicas: 1
       restart_policy:
         condition: on-failure
       placement:
         constraints:
           - node.hostname == akswnc4.aksw.uni-leipzig.de
       labels:
         traefik.docker.network: sansa
         traefik.port: 8080
         traefik.frontend.rule: notebooks.sansa.aksw.org
    ports:
      - 80:8080

networks:
  default:
    external:
      name: sansa 
